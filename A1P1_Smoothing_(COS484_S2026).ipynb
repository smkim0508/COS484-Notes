{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/smkim0508/COS484-Notes/blob/main/A1P1_Smoothing_(COS484_S2026).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TX7_-K31Zp7y"
      },
      "source": [
        "# Notebook for Programming Question 1\n",
        "Welcome to the programming portion of the assignment! Each assignment throughout the semester will have a theory portion and a programming portion. We will be using [Google Colab](https://colab.research.google.com/notebooks/intro.ipynb#recent=true), so if you have never used it before, take a quick look through this introduction: [Working with Google Colab](https://docs.google.com/document/d/1LlnXoOblXwW3YX-0yG_5seTXJsb3kRdMMRYqs8Qqum4/edit?usp=sharing).\n",
        "\n",
        "We'll also be programming in Python, which we will assume a basic familiarity with. Python has fantastic community support and we'll be using numerous packages for machine learning (ML) and natural language processing (NLP) tasks.\n",
        "\n",
        "### Learning Objectives\n",
        "In this problem we will experiment with language models and implement smoothing. We will also see effects of using unigram/bigram LMs and the size of the training data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gPv_JEf8ZvIb"
      },
      "source": [
        "### Data preprocessing\n",
        "\n",
        "In this section, you should write methods to load data and clean (tokenize) it. You will need to write two functions for tokenization. One function, **basicTokenize**, should simply split the text using whitespace. The other function, **nltkTokenize**, should implement NLTK tokenization. Write another function to count the top k most frequent words in a list. You may structure this code however you like, but we suggest constructing a Tokenizer class to encompass these functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2apYXiYZxog",
        "outputId": "cc43b855-5514-4775-eaa6-d2dedd2b4d64",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "class Tokenizer():\n",
        "\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def basicTokenize(self, text: str) -> list[str]:\n",
        "    \"\"\"\n",
        "    splits text by whitespace\n",
        "    \"\"\"\n",
        "    return text.split()\n",
        "\n",
        "  def nltkTokenize(self, text: str) -> list[str]:\n",
        "    \"\"\"\n",
        "    splits token using nltk library\n",
        "    \"\"\"\n",
        "    return nltk.word_tokenize(text)\n",
        "\n",
        "  def count_top_words(self, words: list[str], k: int) -> list[str]:\n",
        "    \"\"\"\n",
        "    counts the top k words and returns the top k as a sorted list\n",
        "    \"\"\"\n",
        "    word_counts = {}\n",
        "    for word in words:\n",
        "      if word in word_counts:\n",
        "        word_counts[word] += 1\n",
        "      else:\n",
        "        word_counts[word] = 1\n",
        "\n",
        "    sorted_word_counts = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)\n",
        "    return sorted_word_counts[:k]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()\n",
        "# test tokenization\n",
        "text = \"This is a test string. I like food.\"\n",
        "basic_tokens = tokenizer.basicTokenize(text)\n",
        "nltk_tokens = tokenizer.nltkTokenize(text)\n",
        "\n",
        "print(f\"Basic Tokenization: {basic_tokens}\")\n",
        "print(f\"NLTK Tokenization: {nltk_tokens}\")"
      ],
      "metadata": {
        "id": "eqTCFJSqzmzd",
        "outputId": "5ca7f097-c673-464c-e56e-e03d4cbb7da2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Basic Tokenization: ['This', 'is', 'a', 'test', 'string.', 'I', 'like', 'food.']\n",
            "NLTK Tokenization: ['This', 'is', 'a', 'test', 'string', '.', 'I', 'like', 'food', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXYBlcTyagBQ"
      },
      "source": [
        "### Language Modeling and Smoothing\n",
        "In this section, you should write methods to train and test a bigram language model. These functions will need to include computing bigram counts, estimating bigram probabilities, and calculating perplexity on the test set. You should also implement a method that can later be called to modify the probabilities with add-alpha smoothing. We suggest encompassing these functions in a LanguageModel class to make experiments easier to run."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BK7uqvcEa49P"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkIAhWqYeUgL"
      },
      "source": [
        "### Instantiate an LM and calculate perplexity\n",
        "Write a wrapper method to train and evaluate a language model on a given train and dev corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nILv3Tyleb54"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8hSR_9VczRI"
      },
      "source": [
        "### Load and tokenize the training and validation data using your code from the Data Processing section\n",
        "\n",
        "You can download training and validation datasets for this problem from the links below:\n",
        "*   Training data: https://princeton-nlp.github.io/cos484/assignments/a1/brown-train.txt\n",
        "*   Validation data: https://princeton-nlp.github.io/cos484/assignments/a1/brown-val.txt\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "syUH84NUc1rL"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtrPOKd1vvyx"
      },
      "source": [
        "## Experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "anwiyViHMCTt"
      },
      "source": [
        "#### Plot the frequency of words\n",
        "Code for sub-part (a)(b)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1xEdVHxMl0n"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**(a) **Report the top 10 words ordered by their frequency in the training corpus, both using basicTokenize and nltkTokenize. What differences do you notice between the two?**\n",
        "\n",
        "TODO: ANSWER THE QUESTION HERE (DOUBLE-CLICK TO EDIT)\n",
        "\n",
        "\n",
        "**(b) Using the nltkTokenize function you wrote, make a plot of the frequencies of words in the training corpus, ordered by their rank, i.e. most frequent word first, the second most word next, and so on on the x axis. Plot only the top 100 most common words to see the trend more clearly. What pattern do you observe in your plot regarding frequency and rank? Do the frequencies follow Zipf's law?**\n",
        "\n",
        "TODO: ANSWER THE QUESTION HERE (DOUBLE-CLICK TO EDIT)"
      ],
      "metadata": {
        "id": "NW8i6tdW5oYi"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DAWeJQ8VdBHU"
      },
      "source": [
        "#### Report the train and test perplexity after learning the language model\n",
        "Code for sub-part (c)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xH79REFCcNRH"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**(c) Train the model and report its perplexity on the train and validation sets. Is the train or val perplexity higher and why?**\n",
        "\n",
        "TODO: ANSWER THE QUESTION HERE (DOUBLE-CLICK TO EDIT)\n",
        "\n",
        "**(c) What do you notice about the val perplexity and why is this the case?**\n",
        "\n",
        "TODO: ANSWER THE QUESTION HERE (DOUBLE-CLICK TO EDIT)"
      ],
      "metadata": {
        "id": "IpU0XsC_7KFo"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4ntdVf2j24K"
      },
      "source": [
        "#### Add-alpha smoothing\n",
        "Code for sub-part (d)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmMeDg-lkQkC"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**(d) Implement Laplace (add-$\\alpha$) smoothing and retrain the model. Plot the perplexity on train and validation sets as a function of alpha (with values $10^{-5}, 10^{-4}, 10^{-3}, 10^{-2}, 10^{-1}, 1, 10$).\n",
        "What happens to the validation and training perplexity as we increase alpha and why does this happen?**\n",
        "\n",
        "TODO: ANSWER THE QUESTION HERE (DOUBLE-CLICK TO EDIT)\n",
        "\n",
        "**(d) What seems to be a good setting for alpha? Provide brief justification.**\n",
        "\n",
        "TODO: ANSWER THE QUESTION HERE (DOUBLE-CLICK TO EDIT)"
      ],
      "metadata": {
        "id": "NcJF8OBH7x2F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**(e) Based on your performance in the previous experiments, propose one idea apart from Laplace smoothing to\n",
        "improve the performance of your bigram language model on the validation set. Briefly describe the modification,\n",
        "explain why you expect it will improve validation perplexity, and discuss any potential limitations.**\n",
        "\n",
        "TODO: ANSWER THE QUESTION HERE (DOUBLE-CLICK TO EDIT)"
      ],
      "metadata": {
        "id": "SgO4kstsjOlv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM Prompts\n",
        "\n",
        "If you used an AI tool to complete any part of this assignment, please paste all prompts you used to produce your final code/responses in the box below and answer the following reflection question."
      ],
      "metadata": {
        "id": "wp7yWTZabCoK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prompts Used:\n",
        "*   \n",
        "*   \n",
        "\n"
      ],
      "metadata": {
        "id": "71OQHYMNbVi9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Reflection: What parts of the AI generated output required modification or improvement? Describe the feedback you gave the tool to produce your final output or any changes you had to make on your own.**\n",
        "\n",
        "TODO: ANSWER THE QUESTION HERE (DOUBLE-CLICK TO EDIT)"
      ],
      "metadata": {
        "id": "KDrwY2DWcTQj"
      }
    }
  ]
}